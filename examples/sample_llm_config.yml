# Sample LLM Configuration File
# This demonstrates the structure expected by the enhanced LLMConfig class
# following SingletonFileLoader best practices

providers:
  # OpenAI Configuration
  openai:
    api_key: "${OPENAI_API_KEY}"  # Use environment variable
    base_url: "https://api.openai.com/v1"
    models:
      - name: "gpt-4"
        model: "gpt-4-0613"
        tags: ["chat", "reasoning", "premium", "multimodal"]
        description: "GPT-4 model for complex reasoning and multimodal tasks"
      
      - name: "gpt-4-turbo"
        model: "gpt-4-1106-preview"
        tags: ["chat", "reasoning", "premium", "long-context"]
        description: "GPT-4 Turbo with 128k context window"
      
      - name: "gpt-3.5-turbo"
        model: "gpt-3.5-turbo-0613"
        tags: ["chat", "fast", "cost-effective"]
        description: "Fast and cost-effective chat model"
      
      - name: "gpt-3.5-turbo-16k"
        model: "gpt-3.5-turbo-16k-0613"
        tags: ["chat", "fast", "cost-effective", "long-context"]
        description: "GPT-3.5 Turbo with extended 16k context"

  # Anthropic Configuration
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"  # Use environment variable
    base_url: "https://api.anthropic.com"
    models:
      - name: "claude-3-opus"
        model: "claude-3-opus-20240229"
        tags: ["chat", "reasoning", "premium", "long-context"]
        description: "Claude 3 Opus for advanced reasoning with 200k context"
      
      - name: "claude-3-sonnet"
        model: "claude-3-sonnet-20240229"
        tags: ["chat", "reasoning", "balanced"]
        description: "Claude 3 Sonnet for balanced performance and cost"
      
      - name: "claude-3-haiku"
        model: "claude-3-haiku-20240307"
        tags: ["chat", "fast", "cost-effective"]
        description: "Claude 3 Haiku for quick responses"

  # Local/Self-hosted Configuration Example
  local:
    api_key: "local-key"  # Not needed for local models but required by schema
    base_url: "http://localhost:8000/v1"
    models:
      - name: "llama-2-7b"
        model: "llama-2-7b-chat"
        tags: ["chat", "local", "privacy", "cost-effective"]
        description: "Local Llama 2 7B model for privacy-focused applications"
      
      - name: "codellama-13b"
        model: "codellama-13b-instruct"
        tags: ["code", "local", "privacy"]
        description: "Local Code Llama 13B for code generation"

  # Azure OpenAI Configuration Example
  azure:
    api_key: "${AZURE_OPENAI_API_KEY}"
    base_url: "https://your-resource.openai.azure.com/"
    models:
      - name: "gpt-4-azure"
        model: "gpt-4"  # Deployment name in Azure
        tags: ["chat", "reasoning", "premium", "enterprise"]
        description: "GPT-4 deployed on Azure OpenAI Service"
      
      - name: "gpt-35-turbo-azure"
        model: "gpt-35-turbo"  # Deployment name in Azure
        tags: ["chat", "fast", "enterprise"]
        description: "GPT-3.5 Turbo deployed on Azure OpenAI Service"

# Configuration Notes:
# 1. Use environment variables for API keys to keep them secure
# 2. Tags help categorize models for easy selection (e.g., get_model_by_tag("fast"))
# 3. The 'model' field should match the API identifier expected by the provider
# 4. The 'name' field is used internally and should be unique within each provider
# 5. Base URLs should include the version path where applicable
# 6. All provider configurations must include: api_key, base_url, models
# 7. All model configurations must include: name, model
# 8. Tags and description are optional but recommended for better organization